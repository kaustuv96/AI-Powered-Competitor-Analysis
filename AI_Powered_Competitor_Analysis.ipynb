{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4814e39-c1c2-4b1e-8ae2-c5b8b49aec46",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201dd84b-007a-435f-bdf4-52dbf15f2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import langchain\n",
    "import transformers\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import PyPDF2\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a15433-026c-4b97-8b61-7e22e21fa554",
   "metadata": {},
   "source": [
    "#### Printing library versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41764921-beb6-423f-b44d-67c9ea7214e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Printing the Versions of Libraries Used\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# Print library versions\n",
    "print(\"Libraries used:\")\n",
    "print(f\"- streamlit: {st.__version__}\")\n",
    "print(f\"- langchain: {langchain.__version__}\")\n",
    "print(f\"- transformers: {transformers.__version__}\")\n",
    "print(f\"- PyPDF2: {PyPDF2.__version__}\")\n",
    "print(\"- re\") # Built-in module, no version\n",
    "print(\"- os\") # Built-in module, no version\n",
    "print(\"- io\") #Built-in module, no version\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Google Generative AI (langchain_google_genai) version: Version information not available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb43be80-996d-4762-9dc7-5f86d9b35846",
   "metadata": {},
   "source": [
    "#### Initial steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbee8d-bb4b-4962-867e-7574d35f7fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Initial Steps\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# --- Streamlit App Configuration ---\n",
    "st.set_page_config(page_title=\"AI-Powered Competitor Analyzer\")\n",
    "\n",
    "# Load API Key\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") # Retrieves the Google API key from the environment variables.\n",
    "if not GOOGLE_API_KEY:\n",
    "    st.error(\"Google API Key is missing! Set the GOOGLE_API_KEY environment variable.\")\n",
    "    st.stop()\n",
    "\n",
    "# --- Initialize Gemini Model ---\n",
    "gemini_llm = GoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7, google_api_key=GOOGLE_API_KEY) # Initializes the Gemini LLM with specified parameters.\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_file): # Defines a function to extract text from PDF files.\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Process long text by splitting and summarizing\n",
    "def process_long_text(text): # Defines a function to process long texts by splitting and summarizing.\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=10000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # For multiple chunks, summarize first\n",
    "    if len(chunks) > 1:\n",
    "        model = get_model()\n",
    "        summaries = []\n",
    "        for chunk in chunks:\n",
    "            response = model.generate_content(f\"Summarize the following text concisely:\\n\\n{chunk}\")\n",
    "            summaries.append(response.text)\n",
    "        \n",
    "        return \" \".join(summaries)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e029181d-9d64-40f9-a54e-9e229b2fdd37",
   "metadata": {},
   "source": [
    "#### Creating prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f58069-d49c-4873-a791-63fcd80b7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt Templates ---\n",
    "competitor_analysis_template = \"\"\"\n",
    "You are an expert market analyst. Provide a competitor analysis based on the provided text. Focus on identifying the key competitors mentioned, \n",
    "their strengths, weaknesses, market strategies, potential opportunities, and areas of differentiation as described in the text. Format each section with clear headings.\n",
    "Also include possible innovation or disruption threats that the competitors face, based on the text. Provide specific examples where possible.\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Competitor Analysis:\n",
    "\"\"\"\n",
    "\n",
    "competitor_prompt = PromptTemplate(input_variables=[\"text\"], template=competitor_analysis_template)\n",
    "competitor_chain = LLMChain(prompt=competitor_prompt, llm=gemini_llm, output_key=\"competitor_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e58e327-3c63-4963-a7c2-ee394bc3b48c",
   "metadata": {},
   "source": [
    "#### Creating Token Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c2530-ef50-46fd-95ee-f700848b8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Token Tracking Variables ---\n",
    "input_tokens_used = 0\n",
    "output_tokens_used = 0\n",
    "\n",
    "# Initialize the tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")  # or a suitable Gemini tokenizer\n",
    "except Exception as e:\n",
    "    st.warning(f\"Failed to load tokenizer: {e}. Token counts will be a rough estimate.\")\n",
    "    tokenizer = None\n",
    "\n",
    "\n",
    "def count_tokens(text: str, tokenizer) -> int:\n",
    "    \"\"\"Counts the number of tokens in a text string using a Hugging Face tokenizer.\"\"\"\n",
    "    if tokenizer:\n",
    "        try:\n",
    "            return len(tokenizer.encode(text))\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Failed to count tokens using tokenizer: {e}. Using a rough estimate (word count).\")\n",
    "            return len(text.split())  # Fallback to a rough estimate if tokenization fails\n",
    "    else:\n",
    "        return len(text.split())  # Fallback to a rough estimate if no tokenizer is available\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def update_token_display(input_used=None, output_used=None):\n",
    "    \"\"\"Updates the token usage display in the sidebar.\"\"\"\n",
    "    markdown_text = \"\"\n",
    "    if input_used is not None:\n",
    "        markdown_text += f\"\"\"**Input Tokens Used:** {input_used:,}  \\n\"\"\"\n",
    "    if output_used is not None:\n",
    "        markdown_text += f\"\"\"**Output Tokens Used:** {output_used:,}\"\"\"\n",
    "\n",
    "    st.sidebar.markdown(markdown_text, unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcc7835-7911-47db-9c87-612be64d6868",
   "metadata": {},
   "source": [
    "#### Setting Up the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026050d7-5bf8-40fd-8c06-952c2c7a7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Streamlit App Layout ---\n",
    "st.title(\"AI-Powered Competitor Analyzer\")\n",
    "st.markdown(\"An AI-powered Agent using Streamlit and LangChain to analyze business information, do competitor analysis.\")\n",
    "\n",
    "st.markdown(\"Powered by Gemini-2.0-Flash, LangChain version: 0.3.20, & Streamlit version: 1.37.1\")\n",
    "st.markdown(\"Transformers version: 4.49.0, Google Generative AI version: Version information not available\")\n",
    "\n",
    "st.markdown(\"Upload a file with competitor data and relevant business information or enter text to generate a competitor analysis.\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Choose a text file\", type=[\"txt\", \"pdf\", \"csv\"])\n",
    "text_input = st.text_area(\"Or enter text directly:\", height=200)\n",
    "\n",
    "\n",
    "text = \"\"  # Initialize text\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    if uploaded_file.type == \"application/pdf\":\n",
    "        text = extract_text_from_pdf(BytesIO(uploaded_file.read()))\n",
    "    else:\n",
    "        try:\n",
    "            text = uploaded_file.read().decode(\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            st.error(\"Error: The uploaded text file is not UTF-8 encoded. Please ensure the file is properly encoded.\")\n",
    "            text = None\n",
    "else:\n",
    "    text = text_input if text_input else None\n",
    "\n",
    "if text:\n",
    "    st.subheader(\"Original Text:\")\n",
    "    st.markdown(f'<div style=\"height: 200px; overflow-y: scroll; border: 1px solid #ccc; padding: 10px;\">{text}</div>', unsafe_allow_html=True)\n",
    "\n",
    "if st.button(\"Analyze Competitors\"):\n",
    "    if text:\n",
    "        with st.spinner(\"Analyzing...\"):\n",
    "            try:\n",
    "                # Track Input Tokens. Text used as Input\n",
    "                input_text = f\"Text: {text}\" #All inputs measured here\n",
    "                input_tokens_used = count_tokens(input_text, tokenizer)\n",
    "                update_token_display(input_used=input_tokens_used)\n",
    "\n",
    "                analysis = competitor_chain.run(text=text) #Pass text to chain\n",
    "\n",
    "                # Track Output Tokens\n",
    "                output_tokens_used = count_tokens(analysis, tokenizer)\n",
    "                update_token_display(output_used=output_tokens_used)\n",
    "\n",
    "                st.subheader(\"Competitor Analysis Report:\")\n",
    "                st.write(analysis)\n",
    "\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error: {e}\")\n",
    "    else:\n",
    "        st.error(\"Please provide text input.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
